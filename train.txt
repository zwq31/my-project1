from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    DataCollatorForSeq2Seq,
    TrainingArguments,
    Trainer,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model
import torch

import wandb 
wandb.login(key="1b8adc705fb9b3e125c05f15107ad7c22c830811")
wandb.init(project="unslothDeepSeek-R1-Distill-Qwen-7B-unsloth-bnb-4bit-cMedQA2-Qlora")

# Step2 加载医疗数据集
ds = load_dataset("cMedQA2")
print(ds["train"][:2])  # 查看数据示例

# Step3 数据集预处理
tokenizer = AutoTokenizer.from_pretrained(
    "unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit",
    padding_side="right",
    use_fast=True
)
tokenizer.pad_token = tokenizer.eos_token  # 确保设置pad_token

def medical_process_func(examples):
    MAX_LENGTH = 512  # 医疗问答较长
    inputs = []
    outputs = []
    
    # 构建医疗问答格式
    for q, a in zip(examples["question"], examples["answer"]):
        inputs.append(f"Human: 你是一名医生，请回答以下问题：{q.strip()}\n\nAssistant: ")
        outputs.append(f"{a.strip()}{tokenizer.eos_token}")
    
    # 批量编码
    model_inputs = tokenizer(inputs, add_special_tokens=False, padding=False, truncation=False)
    labels = tokenizer(outputs, add_special_tokens=False, padding=False, truncation=False)
    
    # 创建结果字典
    full_input_ids = []
    full_attention_masks = []
    full_labels = []
    
    # 拼接输入输出
    for input_ids, label_ids in zip(model_inputs["input_ids"], labels["input_ids"]):
        full_ids = input_ids + label_ids
        attention_mask = [1] * len(full_ids)
        labels = [-100] * len(input_ids) + label_ids
        
        # 截断处理
        if len(full_ids) > MAX_LENGTH:
            full_ids = full_ids[:MAX_LENGTH]
            attention_mask = attention_mask[:MAX_LENGTH]
            labels = labels[:MAX_LENGTH]
        
        full_input_ids.append(full_ids)
        full_attention_masks.append(attention_mask)
        full_labels.append(labels)
    
    # 返回一个字典，每个键是数据集的列名
    return {
        "input_ids": full_input_ids,
        "attention_mask": full_attention_masks,
        "labels": full_labels
    }
tokenized_ds = ds.map(
    medical_process_func,
    batched=True,
    remove_columns=ds["train"].column_names,
    batch_size=100  # 加速处理
)

# Step4 加载4bit量化模型
model = AutoModelForCausalLM.from_pretrained(
    "unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit",
    device_map="auto",
    quantization_config=BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True
    ),
    use_cache=False  # 梯度检查点需要
)

# Step5 配置QLoRA
lora_config = LoraConfig(
    r=32,
    lora_alpha=64,
    target_modules=["q_proj", "v_proj"],  # 适配Llama架构
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()  # 打印可训练参数占比

# Step6 训练参数配置
args = TrainingArguments(
    output_dir="./medical_finetune",
    per_device_train_batch_size=2,          # 根据显存调整
    gradient_accumulation_steps=16,          # 实际batch_size=32
    learning_rate=2e-5,                      # 适合4bit训练
    num_train_epochs=3,
    logging_steps=20,
    evaluation_strategy="no",
    save_strategy="epoch",
    fp16=True,                              # 混合精度训练
    optim="paged_adamw_32bit",              # 优化内存使用
    gradient_checkpointing=True,            # 减少显存占用
    report_to="wandb",
    remove_unused_columns=False             # 保留必要列
)

# Step7 创建训练器
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tokenized_ds["train"],
    data_collator=DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        padding=True,
        pad_to_multiple_of=8  # 优化显存使用
    ),
)

# Step8 开始训练
print("===== 开始训练 =====")
trainer.train()

# Step9 保存适配器
model.save_pretrained("./medical_lora_adapter")

# Step10 推理测试
prompt = "Human: 患者出现持续发热和皮疹，可能是什么原因？\n\nAssistant: "
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

outputs = model.generate(
    **inputs,
    max_new_tokens=256,
    temperature=0.7,
    do_sample=True,
    eos_token_id=tokenizer.eos_token_id
)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
