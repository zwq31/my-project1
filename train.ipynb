{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step1:导入依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\21205\\anaconda3\\envs\\transformers_1\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\21205\\_netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m3407941284\u001b[0m (\u001b[33m3407941284-hdu\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\study\\LLM\\DeepSeek-R1-Distill-Llama-8B-Qlora\\code\\wandb\\run-20250325_171228-av2heprg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/3407941284-hdu/DeepSeek-R1-Distill-Llama-8B-Qlora/runs/av2heprg' target=\"_blank\">comfy-rain-1</a></strong> to <a href='https://wandb.ai/3407941284-hdu/DeepSeek-R1-Distill-Llama-8B-Qlora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/3407941284-hdu/DeepSeek-R1-Distill-Llama-8B-Qlora' target=\"_blank\">https://wandb.ai/3407941284-hdu/DeepSeek-R1-Distill-Llama-8B-Qlora</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/3407941284-hdu/DeepSeek-R1-Distill-Llama-8B-Qlora/runs/av2heprg' target=\"_blank\">https://wandb.ai/3407941284-hdu/DeepSeek-R1-Distill-Llama-8B-Qlora/runs/av2heprg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/3407941284-hdu/DeepSeek-R1-Distill-Llama-8B-Qlora/runs/av2heprg?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1a940f12f80>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb \n",
    "wandb.login(key=\"1b8adc705fb9b3e125c05f15107ad7c22c830811\")\n",
    "wandb.init(project=\"DeepSeek-R1-Distill-Llama-8B-Qlora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step2:模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.70s/it]\n"
     ]
    }
   ],
   "source": [
    "# 定义模型路径\n",
    "model_path = \"D:/study/LLM/DeepSeek-R1-Distill-Llama-8B-Qlora/models/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "# 加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    max_seq_length=2048,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=True\n",
    ")\n",
    "\n",
    "# 确保设置 pad_token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# # 加载模型\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "# model_path,\n",
    "#      device_map=\"auto\",\n",
    "#     use_cache=False  # 梯度检查点需要\n",
    "# )\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, low_cpu_mem_usage=True, \n",
    "                                             torch_dtype=torch.bfloat16, device_map=\"auto\", load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "                                             bnb_4bit_quant_type=\"nf4\", bnb_4bit_use_double_quant=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaTokenizerFast(name_or_path='D:/study/LLM/DeepSeek-R1-Distill-Llama-8B-Qlora/models/DeepSeek-R1-Distill-Llama-8B', vocab_size=128000, model_max_length=16384, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<｜begin▁of▁sentence｜>', 'eos_token': '<｜end▁of▁sentence｜>', 'pad_token': '<｜end▁of▁sentence｜>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t128000: AddedToken(\"<｜begin▁of▁sentence｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128001: AddedToken(\"<｜end▁of▁sentence｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128002: AddedToken(\"<|reserved_special_token_0|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128003: AddedToken(\"<|reserved_special_token_1|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128004: AddedToken(\"<|finetune_right_pad_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128005: AddedToken(\"<|reserved_special_token_2|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128006: AddedToken(\"<|start_header_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128007: AddedToken(\"<|end_header_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128008: AddedToken(\"<|eom_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128009: AddedToken(\"<|eot_id|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128010: AddedToken(\"<|python_tag|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128011: AddedToken(\"<｜User｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t128012: AddedToken(\"<｜Assistant｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t128013: AddedToken(\"<think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t128014: AddedToken(\"</think>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t128015: AddedToken(\"<｜▁pad▁｜>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128016: AddedToken(\"<|reserved_special_token_8|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128017: AddedToken(\"<|reserved_special_token_9|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128018: AddedToken(\"<|reserved_special_token_10|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128019: AddedToken(\"<|reserved_special_token_11|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128020: AddedToken(\"<|reserved_special_token_12|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128021: AddedToken(\"<|reserved_special_token_13|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128022: AddedToken(\"<|reserved_special_token_14|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128023: AddedToken(\"<|reserved_special_token_15|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128024: AddedToken(\"<|reserved_special_token_16|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128025: AddedToken(\"<|reserved_special_token_17|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128026: AddedToken(\"<|reserved_special_token_18|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128027: AddedToken(\"<|reserved_special_token_19|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128028: AddedToken(\"<|reserved_special_token_20|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128029: AddedToken(\"<|reserved_special_token_21|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128030: AddedToken(\"<|reserved_special_token_22|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128031: AddedToken(\"<|reserved_special_token_23|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128032: AddedToken(\"<|reserved_special_token_24|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128033: AddedToken(\"<|reserved_special_token_25|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128034: AddedToken(\"<|reserved_special_token_26|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128035: AddedToken(\"<|reserved_special_token_27|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128036: AddedToken(\"<|reserved_special_token_28|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128037: AddedToken(\"<|reserved_special_token_29|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128038: AddedToken(\"<|reserved_special_token_30|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128039: AddedToken(\"<|reserved_special_token_31|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128040: AddedToken(\"<|reserved_special_token_32|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128041: AddedToken(\"<|reserved_special_token_33|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128042: AddedToken(\"<|reserved_special_token_34|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128043: AddedToken(\"<|reserved_special_token_35|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128044: AddedToken(\"<|reserved_special_token_36|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128045: AddedToken(\"<|reserved_special_token_37|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128046: AddedToken(\"<|reserved_special_token_38|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128047: AddedToken(\"<|reserved_special_token_39|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128048: AddedToken(\"<|reserved_special_token_40|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128049: AddedToken(\"<|reserved_special_token_41|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128050: AddedToken(\"<|reserved_special_token_42|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128051: AddedToken(\"<|reserved_special_token_43|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128052: AddedToken(\"<|reserved_special_token_44|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128053: AddedToken(\"<|reserved_special_token_45|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128054: AddedToken(\"<|reserved_special_token_46|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128055: AddedToken(\"<|reserved_special_token_47|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128056: AddedToken(\"<|reserved_special_token_48|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128057: AddedToken(\"<|reserved_special_token_49|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128058: AddedToken(\"<|reserved_special_token_50|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128059: AddedToken(\"<|reserved_special_token_51|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128060: AddedToken(\"<|reserved_special_token_52|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128061: AddedToken(\"<|reserved_special_token_53|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128062: AddedToken(\"<|reserved_special_token_54|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128063: AddedToken(\"<|reserved_special_token_55|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128064: AddedToken(\"<|reserved_special_token_56|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128065: AddedToken(\"<|reserved_special_token_57|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128066: AddedToken(\"<|reserved_special_token_58|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128067: AddedToken(\"<|reserved_special_token_59|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128068: AddedToken(\"<|reserved_special_token_60|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128069: AddedToken(\"<|reserved_special_token_61|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128070: AddedToken(\"<|reserved_special_token_62|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128071: AddedToken(\"<|reserved_special_token_63|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128072: AddedToken(\"<|reserved_special_token_64|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128073: AddedToken(\"<|reserved_special_token_65|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128074: AddedToken(\"<|reserved_special_token_66|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128075: AddedToken(\"<|reserved_special_token_67|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128076: AddedToken(\"<|reserved_special_token_68|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128077: AddedToken(\"<|reserved_special_token_69|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128078: AddedToken(\"<|reserved_special_token_70|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128079: AddedToken(\"<|reserved_special_token_71|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128080: AddedToken(\"<|reserved_special_token_72|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128081: AddedToken(\"<|reserved_special_token_73|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128082: AddedToken(\"<|reserved_special_token_74|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128083: AddedToken(\"<|reserved_special_token_75|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128084: AddedToken(\"<|reserved_special_token_76|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128085: AddedToken(\"<|reserved_special_token_77|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128086: AddedToken(\"<|reserved_special_token_78|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128087: AddedToken(\"<|reserved_special_token_79|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128088: AddedToken(\"<|reserved_special_token_80|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128089: AddedToken(\"<|reserved_special_token_81|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128090: AddedToken(\"<|reserved_special_token_82|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128091: AddedToken(\"<|reserved_special_token_83|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128092: AddedToken(\"<|reserved_special_token_84|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128093: AddedToken(\"<|reserved_special_token_85|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128094: AddedToken(\"<|reserved_special_token_86|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128095: AddedToken(\"<|reserved_special_token_87|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128096: AddedToken(\"<|reserved_special_token_88|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128097: AddedToken(\"<|reserved_special_token_89|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128098: AddedToken(\"<|reserved_special_token_90|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128099: AddedToken(\"<|reserved_special_token_91|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128100: AddedToken(\"<|reserved_special_token_92|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128101: AddedToken(\"<|reserved_special_token_93|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128102: AddedToken(\"<|reserved_special_token_94|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128103: AddedToken(\"<|reserved_special_token_95|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128104: AddedToken(\"<|reserved_special_token_96|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128105: AddedToken(\"<|reserved_special_token_97|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128106: AddedToken(\"<|reserved_special_token_98|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128107: AddedToken(\"<|reserved_special_token_99|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128108: AddedToken(\"<|reserved_special_token_100|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128109: AddedToken(\"<|reserved_special_token_101|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128110: AddedToken(\"<|reserved_special_token_102|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128111: AddedToken(\"<|reserved_special_token_103|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128112: AddedToken(\"<|reserved_special_token_104|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128113: AddedToken(\"<|reserved_special_token_105|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128114: AddedToken(\"<|reserved_special_token_106|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128115: AddedToken(\"<|reserved_special_token_107|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128116: AddedToken(\"<|reserved_special_token_108|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128117: AddedToken(\"<|reserved_special_token_109|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128118: AddedToken(\"<|reserved_special_token_110|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128119: AddedToken(\"<|reserved_special_token_111|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128120: AddedToken(\"<|reserved_special_token_112|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128121: AddedToken(\"<|reserved_special_token_113|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128122: AddedToken(\"<|reserved_special_token_114|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128123: AddedToken(\"<|reserved_special_token_115|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128124: AddedToken(\"<|reserved_special_token_116|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128125: AddedToken(\"<|reserved_special_token_117|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128126: AddedToken(\"<|reserved_special_token_118|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128127: AddedToken(\"<|reserved_special_token_119|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128128: AddedToken(\"<|reserved_special_token_120|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128129: AddedToken(\"<|reserved_special_token_121|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128130: AddedToken(\"<|reserved_special_token_122|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128131: AddedToken(\"<|reserved_special_token_123|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128132: AddedToken(\"<|reserved_special_token_124|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128133: AddedToken(\"<|reserved_special_token_125|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128134: AddedToken(\"<|reserved_special_token_126|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128135: AddedToken(\"<|reserved_special_token_127|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128136: AddedToken(\"<|reserved_special_token_128|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128137: AddedToken(\"<|reserved_special_token_129|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128138: AddedToken(\"<|reserved_special_token_130|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128139: AddedToken(\"<|reserved_special_token_131|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128140: AddedToken(\"<|reserved_special_token_132|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128141: AddedToken(\"<|reserved_special_token_133|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128142: AddedToken(\"<|reserved_special_token_134|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128143: AddedToken(\"<|reserved_special_token_135|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128144: AddedToken(\"<|reserved_special_token_136|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128145: AddedToken(\"<|reserved_special_token_137|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128146: AddedToken(\"<|reserved_special_token_138|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128147: AddedToken(\"<|reserved_special_token_139|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128148: AddedToken(\"<|reserved_special_token_140|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128149: AddedToken(\"<|reserved_special_token_141|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128150: AddedToken(\"<|reserved_special_token_142|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128151: AddedToken(\"<|reserved_special_token_143|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128152: AddedToken(\"<|reserved_special_token_144|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128153: AddedToken(\"<|reserved_special_token_145|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128154: AddedToken(\"<|reserved_special_token_146|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128155: AddedToken(\"<|reserved_special_token_147|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128156: AddedToken(\"<|reserved_special_token_148|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128157: AddedToken(\"<|reserved_special_token_149|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128158: AddedToken(\"<|reserved_special_token_150|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128159: AddedToken(\"<|reserved_special_token_151|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128160: AddedToken(\"<|reserved_special_token_152|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128161: AddedToken(\"<|reserved_special_token_153|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128162: AddedToken(\"<|reserved_special_token_154|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128163: AddedToken(\"<|reserved_special_token_155|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128164: AddedToken(\"<|reserved_special_token_156|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128165: AddedToken(\"<|reserved_special_token_157|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128166: AddedToken(\"<|reserved_special_token_158|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128167: AddedToken(\"<|reserved_special_token_159|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128168: AddedToken(\"<|reserved_special_token_160|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128169: AddedToken(\"<|reserved_special_token_161|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128170: AddedToken(\"<|reserved_special_token_162|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128171: AddedToken(\"<|reserved_special_token_163|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128172: AddedToken(\"<|reserved_special_token_164|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128173: AddedToken(\"<|reserved_special_token_165|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128174: AddedToken(\"<|reserved_special_token_166|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128175: AddedToken(\"<|reserved_special_token_167|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128176: AddedToken(\"<|reserved_special_token_168|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128177: AddedToken(\"<|reserved_special_token_169|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128178: AddedToken(\"<|reserved_special_token_170|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128179: AddedToken(\"<|reserved_special_token_171|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128180: AddedToken(\"<|reserved_special_token_172|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128181: AddedToken(\"<|reserved_special_token_173|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128182: AddedToken(\"<|reserved_special_token_174|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128183: AddedToken(\"<|reserved_special_token_175|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128184: AddedToken(\"<|reserved_special_token_176|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128185: AddedToken(\"<|reserved_special_token_177|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128186: AddedToken(\"<|reserved_special_token_178|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128187: AddedToken(\"<|reserved_special_token_179|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128188: AddedToken(\"<|reserved_special_token_180|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128189: AddedToken(\"<|reserved_special_token_181|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128190: AddedToken(\"<|reserved_special_token_182|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128191: AddedToken(\"<|reserved_special_token_183|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128192: AddedToken(\"<|reserved_special_token_184|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128193: AddedToken(\"<|reserved_special_token_185|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128194: AddedToken(\"<|reserved_special_token_186|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128195: AddedToken(\"<|reserved_special_token_187|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128196: AddedToken(\"<|reserved_special_token_188|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128197: AddedToken(\"<|reserved_special_token_189|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128198: AddedToken(\"<|reserved_special_token_190|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128199: AddedToken(\"<|reserved_special_token_191|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128200: AddedToken(\"<|reserved_special_token_192|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128201: AddedToken(\"<|reserved_special_token_193|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128202: AddedToken(\"<|reserved_special_token_194|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128203: AddedToken(\"<|reserved_special_token_195|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128204: AddedToken(\"<|reserved_special_token_196|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128205: AddedToken(\"<|reserved_special_token_197|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128206: AddedToken(\"<|reserved_special_token_198|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128207: AddedToken(\"<|reserved_special_token_199|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128208: AddedToken(\"<|reserved_special_token_200|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128209: AddedToken(\"<|reserved_special_token_201|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128210: AddedToken(\"<|reserved_special_token_202|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128211: AddedToken(\"<|reserved_special_token_203|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128212: AddedToken(\"<|reserved_special_token_204|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128213: AddedToken(\"<|reserved_special_token_205|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128214: AddedToken(\"<|reserved_special_token_206|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128215: AddedToken(\"<|reserved_special_token_207|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128216: AddedToken(\"<|reserved_special_token_208|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128217: AddedToken(\"<|reserved_special_token_209|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128218: AddedToken(\"<|reserved_special_token_210|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128219: AddedToken(\"<|reserved_special_token_211|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128220: AddedToken(\"<|reserved_special_token_212|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128221: AddedToken(\"<|reserved_special_token_213|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128222: AddedToken(\"<|reserved_special_token_214|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128223: AddedToken(\"<|reserved_special_token_215|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128224: AddedToken(\"<|reserved_special_token_216|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128225: AddedToken(\"<|reserved_special_token_217|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128226: AddedToken(\"<|reserved_special_token_218|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128227: AddedToken(\"<|reserved_special_token_219|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128228: AddedToken(\"<|reserved_special_token_220|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128229: AddedToken(\"<|reserved_special_token_221|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128230: AddedToken(\"<|reserved_special_token_222|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128231: AddedToken(\"<|reserved_special_token_223|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128232: AddedToken(\"<|reserved_special_token_224|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128233: AddedToken(\"<|reserved_special_token_225|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128234: AddedToken(\"<|reserved_special_token_226|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128235: AddedToken(\"<|reserved_special_token_227|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128236: AddedToken(\"<|reserved_special_token_228|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128237: AddedToken(\"<|reserved_special_token_229|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128238: AddedToken(\"<|reserved_special_token_230|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128239: AddedToken(\"<|reserved_special_token_231|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128240: AddedToken(\"<|reserved_special_token_232|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128241: AddedToken(\"<|reserved_special_token_233|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128242: AddedToken(\"<|reserved_special_token_234|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128243: AddedToken(\"<|reserved_special_token_235|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128244: AddedToken(\"<|reserved_special_token_236|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128245: AddedToken(\"<|reserved_special_token_237|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128246: AddedToken(\"<|reserved_special_token_238|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128247: AddedToken(\"<|reserved_special_token_239|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128248: AddedToken(\"<|reserved_special_token_240|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128249: AddedToken(\"<|reserved_special_token_241|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128250: AddedToken(\"<|reserved_special_token_242|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128251: AddedToken(\"<|reserved_special_token_243|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128252: AddedToken(\"<|reserved_special_token_244|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128253: AddedToken(\"<|reserved_special_token_245|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128254: AddedToken(\"<|reserved_special_token_246|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t128255: AddedToken(\"<|reserved_special_token_247|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.Size([128256, 4096]) torch.bfloat16\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.0.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.1.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.2.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.3.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.4.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.5.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.6.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.7.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.8.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.9.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.10.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.11.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.12.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.13.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.14.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.15.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.16.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.16.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.16.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.16.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.16.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.16.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.16.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.16.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.17.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.17.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.17.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.17.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.17.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.17.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.17.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.17.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.18.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.18.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.18.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.18.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.18.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.18.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.18.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.18.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.18.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.19.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.19.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.19.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.19.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.19.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.19.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.19.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.19.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.19.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.20.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.20.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.20.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.20.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.20.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.20.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.20.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.20.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.20.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.21.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.21.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.21.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.21.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.21.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.21.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.21.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.21.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.21.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.22.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.22.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.22.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.22.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.22.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.22.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.22.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.22.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.22.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.23.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.23.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.23.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.23.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.23.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.23.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.23.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.23.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.23.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.24.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.24.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.24.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.24.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.24.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.24.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.24.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.24.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.24.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.25.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.25.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.25.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.25.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.25.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.25.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.25.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.25.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.25.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.26.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.26.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.26.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.26.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.26.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.26.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.26.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.26.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.26.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.27.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.27.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.27.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.27.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.27.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.27.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.27.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.27.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.27.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.28.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.28.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.28.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.28.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.28.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.28.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.28.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.28.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.28.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.29.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.29.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.29.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.29.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.29.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.29.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.29.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.29.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.29.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.30.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.30.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.30.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.30.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.30.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.30.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.30.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.30.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.30.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.31.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.31.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.31.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.31.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.31.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.31.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.31.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.31.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.layers.31.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "model.norm.weight torch.Size([4096]) torch.bfloat16\n",
      "lm_head.weight torch.Size([128256, 4096]) torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape, param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "### Instruction:\n",
    "You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. Please answer the following medical question.\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>\n",
    "{}\n",
    "</think>\n",
    "{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "### 思考过程：\n",
      "\n",
      "1. **病情分析**：\n",
      "   - 患者已发病5天，腹痛稍有减轻，但仍有发热。\n",
      "   - 体检发现右下腹有压痛的包块。\n",
      "\n",
      "2. **初步判断**：\n",
      "   - 包块的存在提示可能是胰腺炎或其他胰腺相关疾病。\n",
      "   - 压痛性包块可能表示胰腺组织的炎症。\n",
      "\n",
      "3. **进一步检查**：\n",
      "   - **影像学检查**：如超声或CT扫描，确认包块的性质和位置。\n",
      "   - **血液检查**：评估炎症标志物（如CRP、白细胞计数）和肝功能。\n",
      "\n",
      "4. **治疗方案**：\n",
      "   - **抗生素治疗**：根据敏感结果选择药物，通常为第三代 cephalosporin 或 iminogamabin。\n",
      "   - **胰腺支持治疗**：包括营养支持、疼痛管理和休息。\n",
      "   - **外科引导下穿刺**：排除感染性腹膜炎或其他严重感染。\n",
      "\n",
      "5. **监测与随访**：\n",
      "   - 定期复查肝功能和炎症标志物。\n",
      "   - 观察病情进展，必要时调整治疗方案。\n",
      "\n",
      "### 最终答案：\n",
      "\n",
      "在发现右下腹压痛包块的情况下，应立即进行影像学检查（如超声或CT）以确认包块的性质和位置。同时，进行血液检查以评估炎症标志物和肝功能。根据抗生素敏感结果选择合适的抗生素治疗，并考虑胰腺支持治疗，如营养支持和疼痛管理。若有必要，可进行外科引导下穿刺以排除感染性腹膜炎或其他严重感染。定期复查肝功能和炎症标志物，监测病情进展并调整治疗方案。<｜end▁of▁sentence｜>\n"
     ]
    }
   ],
   "source": [
    "question = \"一个患有急性阑尾炎的病人已经发病5天，腹痛稍有减轻但仍然发热，在体检时发现右下腹有压痛的包块，此时应如何处理？\"\n",
    "inputs = tokenizer([prompt_style.format(question, \"\",\"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    attention_mask=inputs.attention_mask,\n",
    "    max_new_tokens=1200,\n",
    "    use_cache=True,\n",
    ")\n",
    "\n",
    "response = tokenizer.batch_decode(outputs)\n",
    "print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step3:数据集加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Question', 'Complex_CoT', 'Response']\n"
     ]
    }
   ],
   "source": [
    "#数据集加载\n",
    "from datasets import load_from_disk\n",
    "dataset = load_from_disk(\"D:/study/LLM/DeepSeek-R1-Distill-Llama-8B-Qlora/data/medical-o1-reasoning-SFT-zh\")\n",
    "print(dataset.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step4:数据集预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据预处理\n",
    "train_prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n",
    "\n",
    "### Instruction:\n",
    "You are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. Please answer the following medical question.\n",
    "\n",
    "### Question:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "<think>\n",
    "{}\n",
    "</think>\n",
    "{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    inputs = examples[\"Question\"]\n",
    "    cots = examples[\"Complex_CoT\"]\n",
    "    outputs = examples[\"Response\"]\n",
    "    texts = []\n",
    "    for input, cot, output in zip(inputs, cots, outputs):\n",
    "        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\n",
    "        \"text\": texts,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"\n",
    "    1. 读取 `Question`, `Complex_CoT`, `Response`\n",
    "    2. 按 `train_prompt_style` 拼接成完整文本\n",
    "    3. 进行 Tokenization\n",
    "    4. 返回 `input_ids`, `attention_mask`, `labels`\n",
    "    \"\"\"\n",
    "    inputs = examples[\"Question\"]\n",
    "    cots = examples[\"Complex_CoT\"]\n",
    "    outputs = examples[\"Response\"]\n",
    "\n",
    "    texts = []\n",
    "    for input, cot, output in zip(inputs, cots, outputs):\n",
    "        text = train_prompt_style.format(input, cot, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "\n",
    "    # Tokenize 处理\n",
    "    model_inputs = tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",  # 设定填充方式\n",
    "        max_length=2048,  # 设定最大长度\n",
    "        truncation=True,  # 超长截断\n",
    "        return_tensors=\"pt\"  # 返回 PyTorch 格式\n",
    "    )\n",
    "\n",
    "    # `labels` = `input_ids`（训练时 Shift Right）\n",
    "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].clone()  # 修改为 clone()\n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理后数据列: ['input_ids', 'attention_mask', 'labels']\n",
      "样本数据: {'input_ids': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 11, 35526, 449, 459, 1988, 430, 5825, 4726, 2317, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 627, 10438, 36864, 11, 1781, 15884, 922, 279, 3488, 323, 1893, 264, 3094, 14656, 30308, 8957, 315, 11555, 311, 6106, 264, 20406, 323, 13687, 2077, 382, 14711, 30151, 512, 2675, 527, 264, 6593, 6335, 449, 11084, 6677, 304, 14830, 33811, 11, 50518, 11, 323, 6514, 9293, 13, 5321, 4320, 279, 2768, 6593, 3488, 382, 14711, 16225, 512, 110747, 54581, 3922, 48044, 16, 93115, 9554, 109780, 19000, 105140, 105343, 65455, 105871, 111935, 43240, 45390, 31809, 37985, 56602, 3922, 46961, 23538, 16937, 31374, 230, 40862, 3922, 103786, 105456, 101734, 106, 27384, 30624, 108686, 3922, 36117, 225, 105150, 89753, 101171, 241, 3922, 40526, 16937, 51109, 8067, 249, 3922, 65455, 105871, 17297, 19361, 35894, 113312, 3922, 111694, 45390, 105871, 57942, 97, 50285, 112950, 1811, 106880, 103429, 111571, 19000, 16325, 103242, 16325, 123594, 64889, 113221, 103429, 27948, 14711, 6075, 512, 128013, 198, 103624, 31809, 109780, 19000, 105140, 36827, 65455, 105871, 17905, 46961, 35287, 98184, 31809, 37985, 56602, 104295, 74245, 72368, 72027, 53901, 3922, 34547, 37507, 75140, 123569, 101171, 241, 68379, 3922, 89753, 35287, 53901, 43240, 101171, 241, 1811, 101067, 101067, 105140, 36827, 111498, 104601, 3922, 88367, 34208, 122539, 104601, 113713, 1811, 102280, 15120, 93115, 114788, 105989, 3922, 103048, 115075, 48634, 22656, 37507, 81258, 16937, 103229, 3922, 105140, 36827, 9554, 122539, 104601, 72027, 103329, 81258, 117976, 87743, 255, 35287, 111006, 3490, 11883, 16325, 103242, 9554, 64936, 27479, 37507, 52030, 3922, 111935, 31809, 37985, 56602, 5486, 88356, 21601, 17905, 46961, 23538, 16937, 31374, 230, 40862, 103138, 98184, 111571, 103397, 126997, 101067, 106837, 65455, 101734, 106, 1811, 31809, 109780, 32335, 119237, 50928, 108787, 105871, 57942, 97, 103429, 3922, 105572, 104514, 122539, 104601, 19000, 33014, 21405, 31938, 223, 37985, 3490, 78388, 88356, 123209, 3922, 65455, 105871, 17297, 114002, 35894, 113312, 103138, 88367, 16937, 82533, 21043, 119992, 9554, 65455, 101734, 106, 1811, 52030, 106155, 103429, 40474, 127443, 109759, 30358, 9554, 104587, 104894, 21043, 101171, 241, 57942, 123, 72027, 71005, 53901, 1811, 118498, 106041, 16325, 103242, 16325, 19361, 105703, 105424, 102210, 54656, 123, 101734, 106, 108966, 122539, 101734, 106, 104587, 88367, 21043, 106884, 120143, 106041, 3490, 50667, 111230, 3922, 65455, 105871, 106583, 35894, 113312, 34208, 105871, 57942, 97, 50285, 112950, 34226, 125164, 116436, 103429, 104724, 102987, 17701, 28037, 65455, 105871, 17297, 103138, 21043, 103668, 67494, 19361, 88367, 21043, 89753, 26130, 58291, 114431, 108, 101734, 105, 11571, 108787, 119395, 40053, 54581, 65455, 34048, 58291, 19817, 230, 34048, 9554, 109759, 30358, 99750, 108208, 3922, 118021, 21043, 19361, 33208, 101171, 241, 16937, 31374, 230, 40862, 113711, 115376, 33035, 45893, 58291, 35894, 113312, 127084, 3490, 120248, 105986, 101067, 101067, 101602, 104237, 113550, 108787, 111571, 103397, 34226, 117207, 60358, 114431, 108, 101734, 105, 9554, 125993, 11571, 120308, 42246, 124116, 28037, 109780, 9554, 8107, 108142, 34208, 105140, 36827, 110105, 9554, 105343, 56602, 34171, 63212, 72238, 3922, 122539, 104601, 88367, 21043, 36668, 63212, 102378, 88367, 123976, 80699, 109177, 108966, 101576, 108, 122539, 116998, 9554, 73597, 252, 104650, 3490, 126850, 108729, 9554, 111571, 103397, 54581, 17905, 52030, 103138, 87502, 46961, 23538, 16937, 31374, 230, 40862, 102420, 59464, 114223, 9554, 122793, 114806, 126534, 16325, 103242, 34226, 115598, 30358, 9554, 103429, 13372, 105187, 103668, 19361, 88367, 21043, 117947, 82317, 34226, 102987, 106643, 33671, 9554, 99750, 108208, 27948, 88356, 124116, 111230, 103138, 107585, 103668, 24946, 118465, 9554, 114431, 108, 101734, 105, 3922, 50928, 120248, 105986, 106596, 65455, 105871, 50285, 112950, 64026, 111935, 35894, 113312, 118498, 109759, 30358, 111571, 103397, 1811, 16325, 103242, 70349, 65455, 103138, 119938, 125993, 88367, 34226, 119292, 14336, 121881, 222, 101734, 106, 529, 58291, 14336, 65455, 101734, 121, 529, 1811, 108787, 103429, 13372, 117032, 54581, 65455, 34048, 109759, 30358, 99750, 108208, 121964, 36117, 225, 101519, 224, 34208, 110063, 112523, 102625, 3490, 123209, 105343, 56602, 34208, 109780, 9554, 33014, 103706, 3922, 105140, 36827, 102420, 122539, 102420, 104601, 3922, 48915, 123296, 101600, 119237, 117976, 17701, 65455, 34048, 109543, 109780, 107029, 112294, 9554, 103048, 115075, 73548, 99337, 74245, 103179, 114099, 102778, 1811, 65455, 101734, 121, 103624, 103429, 13372, 50287, 106155, 116798, 102769, 40862, 115286, 103282, 54581, 9554, 99750, 108208, 109759, 30358, 3922, 36117, 225, 101519, 224, 28037, 111935, 35894, 113312, 3490, 108719, 3922, 120248, 105986, 20321, 95, 118368, 34547, 109836, 110235, 19361, 19483, 103429, 13372, 113644, 34226, 18184, 40862, 108562, 3922, 105424, 102210, 14336, 164, 46981, 164, 75309, 101734, 244, 529, 3922, 44388, 103429, 19000, 16325, 103242, 70349, 96511, 64467, 66201, 106880, 109759, 30358, 99750, 108208, 64026, 112183, 19361, 102987, 34048, 35894, 113312, 127084, 1811, 103282, 75863, 96311, 113, 113333, 35287, 33208, 101171, 241, 34208, 105871, 57942, 97, 50285, 112950, 108787, 111571, 103397, 3490, 118670, 124922, 107774, 21043, 105140, 105343, 122539, 104601, 3922, 124376, 122539, 109177, 17701, 117976, 3922, 109780, 9554, 33014, 103706, 54253, 109088, 107965, 103429, 40474, 106246, 13153, 118498, 99750, 108208, 11571, 106775, 106596, 34547, 37046, 108765, 14336, 164, 46981, 164, 75309, 101734, 244, 529, 103624, 103429, 13372, 116798, 120019, 119292, 9174, 128014, 198, 46281, 16325, 103242, 9554, 64936, 27479, 37507, 52030, 104660, 32938, 54581, 9554, 111571, 103397, 119292, 2118, 164, 46981, 164, 75309, 101734, 244, 99007, 103429, 111571, 1811, 106880, 103429, 111571, 117032, 110105, 19000, 65455, 105871, 3922, 125993, 18184, 43240, 45390, 37985, 56602, 3922, 36117, 225, 105150, 89753, 101171, 241, 3922, 115376, 35894, 113312, 3922, 111694, 45390, 105871, 57942, 97, 50285, 112950, 103786, 46961, 23538, 16937, 31374, 230, 40862, 1811, 122539, 104601, 105842, 30358, 9554, 105140, 105343, 34226, 119237, 124376, 106880, 103429, 111571, 9554, 106246, 3922, 118021, 112213, 103048, 115075, 48634, 105842, 112294, 9554, 103203, 109421, 122374, 1811, 122903, 126534, 16325, 103242, 9554, 80866, 104601, 50338, 109177, 5486, 55228, 249, 122539, 33420, 57942, 123, 9554, 125240, 41007, 72917, 55642, 91495, 127730, 107371, 9554, 125044, 122903, 72917, 67933, 105986, 123594, 64889, 34208, 125240, 9174, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 11, 35526, 449, 459, 1988, 430, 5825, 4726, 2317, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 627, 10438, 36864, 11, 1781, 15884, 922, 279, 3488, 323, 1893, 264, 3094, 14656, 30308, 8957, 315, 11555, 311, 6106, 264, 20406, 323, 13687, 2077, 382, 14711, 30151, 512, 2675, 527, 264, 6593, 6335, 449, 11084, 6677, 304, 14830, 33811, 11, 50518, 11, 323, 6514, 9293, 13, 5321, 4320, 279, 2768, 6593, 3488, 382, 14711, 16225, 512, 110747, 54581, 3922, 48044, 16, 93115, 9554, 109780, 19000, 105140, 105343, 65455, 105871, 111935, 43240, 45390, 31809, 37985, 56602, 3922, 46961, 23538, 16937, 31374, 230, 40862, 3922, 103786, 105456, 101734, 106, 27384, 30624, 108686, 3922, 36117, 225, 105150, 89753, 101171, 241, 3922, 40526, 16937, 51109, 8067, 249, 3922, 65455, 105871, 17297, 19361, 35894, 113312, 3922, 111694, 45390, 105871, 57942, 97, 50285, 112950, 1811, 106880, 103429, 111571, 19000, 16325, 103242, 16325, 123594, 64889, 113221, 103429, 27948, 14711, 6075, 512, 128013, 198, 103624, 31809, 109780, 19000, 105140, 36827, 65455, 105871, 17905, 46961, 35287, 98184, 31809, 37985, 56602, 104295, 74245, 72368, 72027, 53901, 3922, 34547, 37507, 75140, 123569, 101171, 241, 68379, 3922, 89753, 35287, 53901, 43240, 101171, 241, 1811, 101067, 101067, 105140, 36827, 111498, 104601, 3922, 88367, 34208, 122539, 104601, 113713, 1811, 102280, 15120, 93115, 114788, 105989, 3922, 103048, 115075, 48634, 22656, 37507, 81258, 16937, 103229, 3922, 105140, 36827, 9554, 122539, 104601, 72027, 103329, 81258, 117976, 87743, 255, 35287, 111006, 3490, 11883, 16325, 103242, 9554, 64936, 27479, 37507, 52030, 3922, 111935, 31809, 37985, 56602, 5486, 88356, 21601, 17905, 46961, 23538, 16937, 31374, 230, 40862, 103138, 98184, 111571, 103397, 126997, 101067, 106837, 65455, 101734, 106, 1811, 31809, 109780, 32335, 119237, 50928, 108787, 105871, 57942, 97, 103429, 3922, 105572, 104514, 122539, 104601, 19000, 33014, 21405, 31938, 223, 37985, 3490, 78388, 88356, 123209, 3922, 65455, 105871, 17297, 114002, 35894, 113312, 103138, 88367, 16937, 82533, 21043, 119992, 9554, 65455, 101734, 106, 1811, 52030, 106155, 103429, 40474, 127443, 109759, 30358, 9554, 104587, 104894, 21043, 101171, 241, 57942, 123, 72027, 71005, 53901, 1811, 118498, 106041, 16325, 103242, 16325, 19361, 105703, 105424, 102210, 54656, 123, 101734, 106, 108966, 122539, 101734, 106, 104587, 88367, 21043, 106884, 120143, 106041, 3490, 50667, 111230, 3922, 65455, 105871, 106583, 35894, 113312, 34208, 105871, 57942, 97, 50285, 112950, 34226, 125164, 116436, 103429, 104724, 102987, 17701, 28037, 65455, 105871, 17297, 103138, 21043, 103668, 67494, 19361, 88367, 21043, 89753, 26130, 58291, 114431, 108, 101734, 105, 11571, 108787, 119395, 40053, 54581, 65455, 34048, 58291, 19817, 230, 34048, 9554, 109759, 30358, 99750, 108208, 3922, 118021, 21043, 19361, 33208, 101171, 241, 16937, 31374, 230, 40862, 113711, 115376, 33035, 45893, 58291, 35894, 113312, 127084, 3490, 120248, 105986, 101067, 101067, 101602, 104237, 113550, 108787, 111571, 103397, 34226, 117207, 60358, 114431, 108, 101734, 105, 9554, 125993, 11571, 120308, 42246, 124116, 28037, 109780, 9554, 8107, 108142, 34208, 105140, 36827, 110105, 9554, 105343, 56602, 34171, 63212, 72238, 3922, 122539, 104601, 88367, 21043, 36668, 63212, 102378, 88367, 123976, 80699, 109177, 108966, 101576, 108, 122539, 116998, 9554, 73597, 252, 104650, 3490, 126850, 108729, 9554, 111571, 103397, 54581, 17905, 52030, 103138, 87502, 46961, 23538, 16937, 31374, 230, 40862, 102420, 59464, 114223, 9554, 122793, 114806, 126534, 16325, 103242, 34226, 115598, 30358, 9554, 103429, 13372, 105187, 103668, 19361, 88367, 21043, 117947, 82317, 34226, 102987, 106643, 33671, 9554, 99750, 108208, 27948, 88356, 124116, 111230, 103138, 107585, 103668, 24946, 118465, 9554, 114431, 108, 101734, 105, 3922, 50928, 120248, 105986, 106596, 65455, 105871, 50285, 112950, 64026, 111935, 35894, 113312, 118498, 109759, 30358, 111571, 103397, 1811, 16325, 103242, 70349, 65455, 103138, 119938, 125993, 88367, 34226, 119292, 14336, 121881, 222, 101734, 106, 529, 58291, 14336, 65455, 101734, 121, 529, 1811, 108787, 103429, 13372, 117032, 54581, 65455, 34048, 109759, 30358, 99750, 108208, 121964, 36117, 225, 101519, 224, 34208, 110063, 112523, 102625, 3490, 123209, 105343, 56602, 34208, 109780, 9554, 33014, 103706, 3922, 105140, 36827, 102420, 122539, 102420, 104601, 3922, 48915, 123296, 101600, 119237, 117976, 17701, 65455, 34048, 109543, 109780, 107029, 112294, 9554, 103048, 115075, 73548, 99337, 74245, 103179, 114099, 102778, 1811, 65455, 101734, 121, 103624, 103429, 13372, 50287, 106155, 116798, 102769, 40862, 115286, 103282, 54581, 9554, 99750, 108208, 109759, 30358, 3922, 36117, 225, 101519, 224, 28037, 111935, 35894, 113312, 3490, 108719, 3922, 120248, 105986, 20321, 95, 118368, 34547, 109836, 110235, 19361, 19483, 103429, 13372, 113644, 34226, 18184, 40862, 108562, 3922, 105424, 102210, 14336, 164, 46981, 164, 75309, 101734, 244, 529, 3922, 44388, 103429, 19000, 16325, 103242, 70349, 96511, 64467, 66201, 106880, 109759, 30358, 99750, 108208, 64026, 112183, 19361, 102987, 34048, 35894, 113312, 127084, 1811, 103282, 75863, 96311, 113, 113333, 35287, 33208, 101171, 241, 34208, 105871, 57942, 97, 50285, 112950, 108787, 111571, 103397, 3490, 118670, 124922, 107774, 21043, 105140, 105343, 122539, 104601, 3922, 124376, 122539, 109177, 17701, 117976, 3922, 109780, 9554, 33014, 103706, 54253, 109088, 107965, 103429, 40474, 106246, 13153, 118498, 99750, 108208, 11571, 106775, 106596, 34547, 37046, 108765, 14336, 164, 46981, 164, 75309, 101734, 244, 529, 103624, 103429, 13372, 116798, 120019, 119292, 9174, 128014, 198, 46281, 16325, 103242, 9554, 64936, 27479, 37507, 52030, 104660, 32938, 54581, 9554, 111571, 103397, 119292, 2118, 164, 46981, 164, 75309, 101734, 244, 99007, 103429, 111571, 1811, 106880, 103429, 111571, 117032, 110105, 19000, 65455, 105871, 3922, 125993, 18184, 43240, 45390, 37985, 56602, 3922, 36117, 225, 105150, 89753, 101171, 241, 3922, 115376, 35894, 113312, 3922, 111694, 45390, 105871, 57942, 97, 50285, 112950, 103786, 46961, 23538, 16937, 31374, 230, 40862, 1811, 122539, 104601, 105842, 30358, 9554, 105140, 105343, 34226, 119237, 124376, 106880, 103429, 111571, 9554, 106246, 3922, 118021, 112213, 103048, 115075, 48634, 105842, 112294, 9554, 103203, 109421, 122374, 1811, 122903, 126534, 16325, 103242, 9554, 80866, 104601, 50338, 109177, 5486, 55228, 249, 122539, 33420, 57942, 123, 9554, 125240, 41007, 72917, 55642, 91495, 127730, 107371, 9554, 125044, 122903, 72917, 67933, 105986, 123594, 64889, 34208, 125240, 9174, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001]}\n"
     ]
    }
   ],
   "source": [
    "# 处理数据集\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True, remove_columns=[\"Question\", \"Complex_CoT\", \"Response\"])\n",
    "\n",
    "# 查看处理后的数据列\n",
    "print(\"处理后数据列:\", dataset.column_names)\n",
    "\n",
    "# 检查第一条样本\n",
    "print(\"样本数据:\", dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 24772\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step5:lora配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, inference_mode=False, r=8, target_modules=None, exclude_modules=None, lora_alpha=8, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "config = LoraConfig(task_type=TaskType.CAUSAL_LM,)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.embed_tokens.weight torch.Size([128256, 4096]) torch.bfloat16\n",
      "base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) torch.float32\n",
      "base_model.model.model.layers.0.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) torch.float32\n",
      "base_model.model.model.layers.0.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.0.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.0.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.0.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.0.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.0.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) torch.float32\n",
      "base_model.model.model.layers.1.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.1.self_attn.v_proj.base_layer.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) torch.float32\n",
      "base_model.model.model.layers.1.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.1.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.1.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.1.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.1.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.1.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.2.self_attn.q_proj.base_layer.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) torch.float32\n",
      "base_model.model.model.layers.2.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.2.self_attn.v_proj.base_layer.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) torch.float32\n",
      "base_model.model.model.layers.2.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.2.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.2.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.2.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.2.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.2.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.3.self_attn.q_proj.base_layer.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) torch.float32\n",
      "base_model.model.model.layers.3.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.3.self_attn.v_proj.base_layer.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) torch.float32\n",
      "base_model.model.model.layers.3.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.3.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.3.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.3.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.3.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.3.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.4.self_attn.q_proj.base_layer.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) torch.float32\n",
      "base_model.model.model.layers.4.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.4.self_attn.v_proj.base_layer.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) torch.float32\n",
      "base_model.model.model.layers.4.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.4.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.4.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.4.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.4.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.4.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.5.self_attn.q_proj.base_layer.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) torch.float32\n",
      "base_model.model.model.layers.5.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.5.self_attn.v_proj.base_layer.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) torch.float32\n",
      "base_model.model.model.layers.5.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.5.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.5.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.5.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.5.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.5.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.6.self_attn.q_proj.base_layer.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) torch.float32\n",
      "base_model.model.model.layers.6.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.6.self_attn.v_proj.base_layer.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) torch.float32\n",
      "base_model.model.model.layers.6.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.6.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.6.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.6.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.6.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.6.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.7.self_attn.q_proj.base_layer.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) torch.float32\n",
      "base_model.model.model.layers.7.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.7.self_attn.v_proj.base_layer.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) torch.float32\n",
      "base_model.model.model.layers.7.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.7.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.7.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.7.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.7.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.7.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.8.self_attn.q_proj.base_layer.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) torch.float32\n",
      "base_model.model.model.layers.8.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.8.self_attn.v_proj.base_layer.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) torch.float32\n",
      "base_model.model.model.layers.8.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.8.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.8.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.8.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.8.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.8.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.9.self_attn.q_proj.base_layer.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) torch.float32\n",
      "base_model.model.model.layers.9.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.9.self_attn.v_proj.base_layer.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) torch.float32\n",
      "base_model.model.model.layers.9.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.9.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.9.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.9.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.9.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.9.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.10.self_attn.q_proj.base_layer.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) torch.float32\n",
      "base_model.model.model.layers.10.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.10.self_attn.v_proj.base_layer.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) torch.float32\n",
      "base_model.model.model.layers.10.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.10.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.10.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.10.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.10.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.10.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.11.self_attn.q_proj.base_layer.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) torch.float32\n",
      "base_model.model.model.layers.11.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.11.self_attn.v_proj.base_layer.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) torch.float32\n",
      "base_model.model.model.layers.11.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.11.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.11.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.11.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.11.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.11.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.12.self_attn.q_proj.base_layer.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) torch.float32\n",
      "base_model.model.model.layers.12.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.12.self_attn.v_proj.base_layer.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) torch.float32\n",
      "base_model.model.model.layers.12.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.12.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.12.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.12.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.12.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.12.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.13.self_attn.q_proj.base_layer.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) torch.float32\n",
      "base_model.model.model.layers.13.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.13.self_attn.v_proj.base_layer.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) torch.float32\n",
      "base_model.model.model.layers.13.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.13.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.13.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.13.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.13.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.13.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.14.self_attn.q_proj.base_layer.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) torch.float32\n",
      "base_model.model.model.layers.14.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.14.self_attn.v_proj.base_layer.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) torch.float32\n",
      "base_model.model.model.layers.14.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.14.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.14.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.14.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.14.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.14.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.15.self_attn.q_proj.base_layer.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) torch.float32\n",
      "base_model.model.model.layers.15.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.15.self_attn.v_proj.base_layer.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) torch.float32\n",
      "base_model.model.model.layers.15.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.15.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.15.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.15.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.15.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.15.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.16.self_attn.q_proj.base_layer.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) torch.float32\n",
      "base_model.model.model.layers.16.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.16.self_attn.v_proj.base_layer.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) torch.float32\n",
      "base_model.model.model.layers.16.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.16.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.16.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.16.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.16.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.16.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.17.self_attn.q_proj.base_layer.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) torch.float32\n",
      "base_model.model.model.layers.17.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.17.self_attn.v_proj.base_layer.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) torch.float32\n",
      "base_model.model.model.layers.17.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.17.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.17.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.17.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.17.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.17.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.18.self_attn.q_proj.base_layer.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) torch.float32\n",
      "base_model.model.model.layers.18.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.18.self_attn.v_proj.base_layer.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) torch.float32\n",
      "base_model.model.model.layers.18.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.18.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.18.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.18.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.18.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.18.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.19.self_attn.q_proj.base_layer.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) torch.float32\n",
      "base_model.model.model.layers.19.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.19.self_attn.v_proj.base_layer.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) torch.float32\n",
      "base_model.model.model.layers.19.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.19.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.19.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.19.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.19.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.19.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.20.self_attn.q_proj.base_layer.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) torch.float32\n",
      "base_model.model.model.layers.20.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.20.self_attn.v_proj.base_layer.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) torch.float32\n",
      "base_model.model.model.layers.20.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.20.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.20.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.20.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.20.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.20.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.21.self_attn.q_proj.base_layer.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) torch.float32\n",
      "base_model.model.model.layers.21.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.21.self_attn.v_proj.base_layer.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) torch.float32\n",
      "base_model.model.model.layers.21.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.21.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.21.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.21.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.21.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.21.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.22.self_attn.q_proj.base_layer.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) torch.float32\n",
      "base_model.model.model.layers.22.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.22.self_attn.v_proj.base_layer.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) torch.float32\n",
      "base_model.model.model.layers.22.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.22.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.22.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.22.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.22.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.22.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.23.self_attn.q_proj.base_layer.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) torch.float32\n",
      "base_model.model.model.layers.23.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.23.self_attn.v_proj.base_layer.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) torch.float32\n",
      "base_model.model.model.layers.23.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.23.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.23.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.23.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.23.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.23.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.24.self_attn.q_proj.base_layer.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) torch.float32\n",
      "base_model.model.model.layers.24.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.24.self_attn.v_proj.base_layer.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) torch.float32\n",
      "base_model.model.model.layers.24.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.24.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.24.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.24.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.24.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.24.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.25.self_attn.q_proj.base_layer.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) torch.float32\n",
      "base_model.model.model.layers.25.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.25.self_attn.v_proj.base_layer.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) torch.float32\n",
      "base_model.model.model.layers.25.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.25.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.25.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.25.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.25.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.25.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.26.self_attn.q_proj.base_layer.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) torch.float32\n",
      "base_model.model.model.layers.26.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.26.self_attn.v_proj.base_layer.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) torch.float32\n",
      "base_model.model.model.layers.26.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.26.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.26.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.26.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.26.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.26.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.27.self_attn.q_proj.base_layer.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) torch.float32\n",
      "base_model.model.model.layers.27.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.27.self_attn.v_proj.base_layer.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) torch.float32\n",
      "base_model.model.model.layers.27.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.27.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.27.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.27.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.27.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.27.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.28.self_attn.q_proj.base_layer.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) torch.float32\n",
      "base_model.model.model.layers.28.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.28.self_attn.v_proj.base_layer.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) torch.float32\n",
      "base_model.model.model.layers.28.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.28.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.28.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.28.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.28.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.28.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.29.self_attn.q_proj.base_layer.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) torch.float32\n",
      "base_model.model.model.layers.29.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.29.self_attn.v_proj.base_layer.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) torch.float32\n",
      "base_model.model.model.layers.29.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.29.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.29.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.29.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.29.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.29.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.30.self_attn.q_proj.base_layer.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) torch.float32\n",
      "base_model.model.model.layers.30.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.30.self_attn.v_proj.base_layer.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) torch.float32\n",
      "base_model.model.model.layers.30.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.30.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.30.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.30.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.30.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.30.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.31.self_attn.q_proj.base_layer.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 8]) torch.float32\n",
      "base_model.model.model.layers.31.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.31.self_attn.v_proj.base_layer.weight torch.Size([2097152, 1]) torch.uint8\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight torch.Size([8, 4096]) torch.float32\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 8]) torch.float32\n",
      "base_model.model.model.layers.31.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "base_model.model.model.layers.31.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.31.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.31.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "base_model.model.model.layers.31.input_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.layers.31.post_attention_layernorm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.model.norm.weight torch.Size([4096]) torch.bfloat16\n",
      "base_model.model.lm_head.weight torch.Size([128256, 4096]) torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape, param.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enable_input_require_grads() # 开启梯度检查点时，要执行该方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.0424\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step6:配置训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"../output\",\n",
    "    per_device_train_batch_size=3,\n",
    "    gradient_accumulation_steps=1,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,                              # 混合精度训练\n",
    "    optim=\"paged_adamw_32bit\",              # 优化内存使用\n",
    "    report_to=\"wandb\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step7:创建训练器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\21205\\AppData\\Local\\Temp\\ipykernel_159512\\924047804.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "   data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True, max_length=2048)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step8:模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "c:\\Users\\21205\\anaconda3\\envs\\transformers_1\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2718: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='71' max='24774' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   71/24774 08:50 < 52:44:31, 0.13 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>16.663200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.143300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.751500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.729600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.699100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.626600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step9:微调模型验证推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"一个患有急性阑尾炎的病人已经发病5天，腹痛稍有减轻但仍然发热，在体检时发现右下腹有压痛的包块，此时应如何处理？\"\n",
    "\n",
    "# model.eval()\n",
    "# inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# outputs = model.generate(\n",
    "#     input_ids=inputs.input_ids,\n",
    "#     attention_mask=inputs.attention_mask,\n",
    "#     max_new_tokens=1200,\n",
    "#     use_cache=True,\n",
    "# )\n",
    "\n",
    "# response = tokenizer.batch_decode(outputs)\n",
    "# print(response[0].split(\"### Response:\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step10:模型保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained(\"../output/medical_lora_adapter\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
