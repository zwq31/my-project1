{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step1:导入依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\21205\\anaconda3\\envs\\transformers_1\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\21205\\_netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m3407941284\u001b[0m (\u001b[33m3407941284-hdu\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\study\\LLM\\unslothDeepSeek-R1-Distill-Qwen-7B-unsloth-bnb-4bit-cMedQA2-Qlora\\code\\wandb\\run-20250323_175525-4wu6cd65</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/3407941284-hdu/unslothDeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit-cMedQA2-Qlora/runs/4wu6cd65' target=\"_blank\">cerulean-pond-1</a></strong> to <a href='https://wandb.ai/3407941284-hdu/unslothDeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit-cMedQA2-Qlora' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/3407941284-hdu/unslothDeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit-cMedQA2-Qlora' target=\"_blank\">https://wandb.ai/3407941284-hdu/unslothDeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit-cMedQA2-Qlora</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/3407941284-hdu/unslothDeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit-cMedQA2-Qlora/runs/4wu6cd65' target=\"_blank\">https://wandb.ai/3407941284-hdu/unslothDeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit-cMedQA2-Qlora/runs/4wu6cd65</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/3407941284-hdu/unslothDeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit-cMedQA2-Qlora/runs/4wu6cd65?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2979229ec20>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb \n",
    "wandb.login(key=\"1b8adc705fb9b3e125c05f15107ad7c22c830811\")\n",
    "wandb.init(project=\"unslothDeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit-cMedQA2-Qlora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step2：数据集加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 188490\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 7527\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 7552\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_from_disk(\"../data/cMedQA2/deduplicate_neg\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': ['不是说做b超对宝宝不好吗？那怀孕检查是不？不是说做b超对宝宝不好吗？那怀孕检查是不是越少越好。无麻烦解答，谢谢。',\n",
       "  '不是说做b超对宝宝不好吗？那怀孕检查是不？不是说做b超对宝宝不好吗？那怀孕检查是不是越少越好。无麻烦解答，谢谢。'],\n",
       " 'answer': ['B超属于超声波经常检查是不好的而且也没有必要经常检查的一般怀孕两个月检查一下怀孕五个月检查一下快出生时在检查就可以还有就是不舒服检查就可以的',\n",
       "  'b超切实有一定的辐射，而且小孩比较的娇嫩，容易受辐射影响发育。宝宝尽量不要做b超，但是在胎儿期有母体的保护，所以不要担心，有必要的话一定要做。']}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step3:数据集预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"D:/study/LLM/unslothDeepSeek-R1-Distill-Qwen-7B-unsloth-bnb-4bit-cMedQA2-Qlora/model/unsloth-DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",\n",
    "    padding_side=\"right\",\n",
    "    use_fast=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # 确保设置pad_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def medical_process_func(examples):\n",
    "    MAX_LENGTH = 512  # 医疗问答较长\n",
    "    inputs = []\n",
    "    outputs = []\n",
    "    \n",
    "    # 构建医疗问答格式\n",
    "    for q, a in zip(examples[\"question\"], examples[\"answer\"]):\n",
    "        inputs.append(f\"Human: 你是一名医生，请回答以下问题：{q.strip()}\\n\\nAssistant: \")\n",
    "        outputs.append(f\"{a.strip()}{tokenizer.eos_token}\")\n",
    "    \n",
    "    # 批量编码\n",
    "    model_inputs = tokenizer(inputs, add_special_tokens=False, padding=False, truncation=False)\n",
    "    labels = tokenizer(outputs, add_special_tokens=False, padding=False, truncation=False)\n",
    "    \n",
    "    # 创建结果字典\n",
    "    full_input_ids = []\n",
    "    full_attention_masks = []\n",
    "    full_labels = []\n",
    "    \n",
    "    # 拼接输入输出\n",
    "    for input_ids, label_ids in zip(model_inputs[\"input_ids\"], labels[\"input_ids\"]):\n",
    "        full_ids = input_ids + label_ids\n",
    "        attention_mask = [1] * len(full_ids)\n",
    "        labels = [-100] * len(input_ids) + label_ids\n",
    "        \n",
    "        # 截断处理\n",
    "        if len(full_ids) > MAX_LENGTH:\n",
    "            full_ids = full_ids[:MAX_LENGTH]\n",
    "            attention_mask = attention_mask[:MAX_LENGTH]\n",
    "            labels = labels[:MAX_LENGTH]\n",
    "        \n",
    "        full_input_ids.append(full_ids)\n",
    "        full_attention_masks.append(attention_mask)\n",
    "        full_labels.append(labels)\n",
    "    \n",
    "    # 返回一个字典，每个键是数据集的列名\n",
    "    return {\n",
    "        \"input_ids\": full_input_ids,\n",
    "        \"attention_mask\": full_attention_masks,\n",
    "        \"labels\": full_labels\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_ds = ds.map(\n",
    "    medical_process_func,\n",
    "    batched=True,\n",
    "    remove_columns=ds[\"train\"].column_names,\n",
    "    batch_size=100  # 加速处理\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 188490\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 7527\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 7552\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step4:模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"D:/study/LLM/unslothDeepSeek-R1-Distill-Qwen-7B-unsloth-bnb-4bit-cMedQA2-Qlora/model/unsloth-DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\",\n",
    "     device_map=\"auto\",\n",
    "    use_cache=False  # 梯度检查点需要\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: 不是说做b超对宝宝不好吗？那怀孕检查是不？不是说做b超对宝宝不好吗？那怀孕检查是不是越少越好。无麻烦解答，谢谢。\n",
      "\n",
      "Assistant: 根据你的描述看起来你可能是在关心怀孯时B超的问题了。如果怀孾时间较长的话，一般每两个星期进行一次B超即可，这样可以防止出现发育异常的情况。但也不能盲目多次的去医院，如果发现不正常就及时处理一下\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Human: 不是说做b超对宝宝不好吗？那怀孕检查是不？不是说做b超对宝宝不好吗？那怀孕检查是不是越少越好。无麻烦解答，谢谢。\\n\\nAssistant: \"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=1024,  # 允许回答更长\n",
    "    temperature=1.0,  # 让回答更加自由\n",
    "    top_p=0.9,  # 让模型考虑更多可能性\n",
    "    top_k=50,  # 增加多样性\n",
    "    repetition_penalty=1.2,  # 防止重复\n",
    "    no_repeat_ngram_size=3,  # 避免生成相同的三元组短语\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 成功加载 LoRA 适配器！\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "model_qlora = PeftModel.from_pretrained(model, \"../output/medical_lora_adapter\")\n",
    "print(\"✅ 成功加载 LoRA 适配器！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: 不是说做b超对宝宝不好吗？那怀孕检查是不？不是说做b超对宝宝不好吗？那怀孕检查是不是越少越好。无麻烦解答，谢谢。\n",
      "\n",
      "Assistant: 早期检查有利于监测胎儿发育情况，是为了发现并及时处理那些可影响后续妊娠发展的病情。在3个月以后进行产前诊断也是很重要的，如果孩子有什么遗传性疾病或者器官畸形就可以通过X光等方法提前治疗或防范，对母子都是非常必要和不可缺少的一部分\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Human: 不是说做b超对宝宝不好吗？那怀孕检查是不？不是说做b超对宝宝不好吗？那怀孕检查是不是越少越好。无麻烦解答，谢谢。\\n\\nAssistant: \"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model_qlora.device)\n",
    "\n",
    "outputs = model_qlora.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=1024,  # 允许回答更长\n",
    "    temperature=1.0,  # 让回答更加自由\n",
    "    top_p=0.9,  # 让模型考虑更多可能性\n",
    "    top_k=50,  # 增加多样性\n",
    "    repetition_penalty=1.2,  # 防止重复\n",
    "    no_repeat_ngram_size=3,  # 避免生成相同的三元组短语\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n",
       "    (layers): ModuleList(\n",
       "      (0): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (1): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (2-31): 30 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.Size([128256, 4096]) torch.float16\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.0.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.1.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.1.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.1.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.1.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.1.mlp.gate_proj.weight torch.Size([14336, 4096]) torch.float16\n",
      "model.layers.1.mlp.up_proj.weight torch.Size([14336, 4096]) torch.float16\n",
      "model.layers.1.mlp.down_proj.weight torch.Size([4096, 14336]) torch.float16\n",
      "model.layers.1.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.1.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.2.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.2.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.2.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.2.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.2.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.2.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.2.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.2.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.2.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.3.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.3.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.3.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.3.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.3.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.3.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.3.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.3.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.3.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.4.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.4.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.4.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.4.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.4.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.4.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.4.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.4.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.4.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.5.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.5.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.5.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.5.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.5.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.5.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.5.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.5.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.5.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.6.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.6.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.6.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.6.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.6.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.6.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.6.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.6.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.6.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.7.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.7.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.7.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.7.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.7.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.7.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.7.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.7.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.7.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.8.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.8.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.8.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.8.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.8.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.8.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.8.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.8.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.8.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.9.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.9.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.9.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.9.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.9.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.9.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.9.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.9.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.9.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.10.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.10.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.10.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.10.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.10.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.10.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.10.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.10.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.10.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.11.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.11.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.11.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.11.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.11.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.11.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.11.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.11.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.11.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.12.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.12.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.12.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.12.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.12.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.12.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.12.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.12.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.12.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.13.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.13.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.13.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.13.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.13.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.13.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.13.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.13.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.13.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.14.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.14.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.14.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.14.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.14.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.14.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.14.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.14.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.14.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.15.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.15.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.15.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.15.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.15.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.15.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.15.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.15.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.15.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.16.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.16.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.16.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.16.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.16.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.16.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.16.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.16.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.16.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.17.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.17.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.17.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.17.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.17.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.17.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.17.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.17.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.17.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.18.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.18.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.18.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.18.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.18.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.18.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.18.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.18.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.18.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.19.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.19.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.19.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.19.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.19.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.19.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.19.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.19.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.19.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.20.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.20.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.20.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.20.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.20.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.20.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.20.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.20.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.20.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.21.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.21.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.21.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.21.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.21.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.21.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.21.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.21.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.21.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.22.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.22.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.22.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.22.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.22.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.22.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.22.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.22.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.22.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.23.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.23.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.23.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.23.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.23.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.23.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.23.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.23.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.23.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.24.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.24.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.24.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.24.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.24.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.24.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.24.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.24.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.24.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.25.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.25.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.25.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.25.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.25.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.25.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.25.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.25.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.25.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.26.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.26.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.26.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.26.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.26.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.26.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.26.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.26.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.26.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.27.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.27.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.27.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.27.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.27.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.27.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.27.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.27.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.27.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.28.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.28.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.28.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.28.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.28.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.28.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.28.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.28.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.28.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.29.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.29.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.29.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.29.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.29.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.29.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.29.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.29.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.29.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.30.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.30.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.30.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.30.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.30.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.30.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.30.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.30.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.30.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.31.self_attn.q_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.31.self_attn.k_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.31.self_attn.v_proj.weight torch.Size([2097152, 1]) torch.uint8\n",
      "model.layers.31.self_attn.o_proj.weight torch.Size([8388608, 1]) torch.uint8\n",
      "model.layers.31.mlp.gate_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.31.mlp.up_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.31.mlp.down_proj.weight torch.Size([29360128, 1]) torch.uint8\n",
      "model.layers.31.input_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.layers.31.post_attention_layernorm.weight torch.Size([4096]) torch.float16\n",
      "model.norm.weight torch.Size([4096]) torch.float16\n",
      "lm_head.weight torch.Size([128256, 4096]) torch.float16\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape, param.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step5:配置QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 13,631,488 || all params: 8,043,892,736 || trainable%: 0.1695\n"
     ]
    }
   ],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # 适配Llama架构\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()  # 打印可训练参数占比"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step5:配置训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\21205\\anaconda3\\envs\\transformers_1\\lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"../output\",\n",
    "    per_device_train_batch_size=4,          # 根据显存调整\n",
    "    gradient_accumulation_steps=4,          # 实际batch_size=32\n",
    "    learning_rate=2e-5,                      # 适合4bit训练\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=20,\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_strategy=\"epoch\",\n",
    "    gradient_checkpointing=False,  # 禁用梯度检查点\n",
    "    fp16=True,                              # 混合精度训练\n",
    "    optim=\"paged_adamw_32bit\",              # 优化内存使用\n",
    "    report_to=\"wandb\",\n",
    "    remove_unused_columns=False   # 保留必要列\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step6:创建训练器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_ds[\"train\"].select(range(50000)),\n",
    "    data_collator=DataCollatorForSeq2Seq(\n",
    "        tokenizer=tokenizer,\n",
    "        padding=True,\n",
    "        pad_to_multiple_of=8  # 优化显存使用\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step7:开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9375' max='9375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9375/9375 7:59:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.224800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.019400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.816900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.694200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.665900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.690200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.633500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.587600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.588300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.571600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.552500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.486900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.533300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.518100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.559000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>2.497400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>2.467500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.484900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>2.491700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.518300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>2.505100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>2.484700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>2.536500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>2.497200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.466000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>2.513300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>2.454300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>2.498100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>2.470800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.526400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>2.462600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>2.494800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>2.438500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>2.462600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.484400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>2.476700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>2.474700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>2.484400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>2.445100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.468200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>2.460700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>2.423000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>2.414200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>2.428500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.435600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>2.404800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>2.453100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>2.424900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>2.483700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.427200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>2.438600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>2.423800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>2.472700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>2.400500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.439400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>2.383100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1140</td>\n",
       "      <td>2.408900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1160</td>\n",
       "      <td>2.448700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1180</td>\n",
       "      <td>2.409300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.342200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1220</td>\n",
       "      <td>2.410100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1240</td>\n",
       "      <td>2.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1260</td>\n",
       "      <td>2.349400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1280</td>\n",
       "      <td>2.403500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.421100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1320</td>\n",
       "      <td>2.431600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1340</td>\n",
       "      <td>2.441800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1360</td>\n",
       "      <td>2.399400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1380</td>\n",
       "      <td>2.416500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.440700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1420</td>\n",
       "      <td>2.398600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1440</td>\n",
       "      <td>2.397300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1460</td>\n",
       "      <td>2.418500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1480</td>\n",
       "      <td>2.379200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1520</td>\n",
       "      <td>2.430800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1540</td>\n",
       "      <td>2.409500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1560</td>\n",
       "      <td>2.411700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1580</td>\n",
       "      <td>2.407900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.413700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1620</td>\n",
       "      <td>2.390200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1640</td>\n",
       "      <td>2.370300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1660</td>\n",
       "      <td>2.417500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1680</td>\n",
       "      <td>2.400600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.387100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1720</td>\n",
       "      <td>2.388700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1740</td>\n",
       "      <td>2.415800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1760</td>\n",
       "      <td>2.363700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1780</td>\n",
       "      <td>2.365600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.355300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>2.400600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1840</td>\n",
       "      <td>2.429500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1860</td>\n",
       "      <td>2.390600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1880</td>\n",
       "      <td>2.401900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.408800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1920</td>\n",
       "      <td>2.385800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1940</td>\n",
       "      <td>2.360800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1960</td>\n",
       "      <td>2.359200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1980</td>\n",
       "      <td>2.374500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.357000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2020</td>\n",
       "      <td>2.381600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2040</td>\n",
       "      <td>2.366100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2060</td>\n",
       "      <td>2.344700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2080</td>\n",
       "      <td>2.367900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.407300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2120</td>\n",
       "      <td>2.394700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2140</td>\n",
       "      <td>2.378200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2160</td>\n",
       "      <td>2.399300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2180</td>\n",
       "      <td>2.342800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.394300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2220</td>\n",
       "      <td>2.371000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2240</td>\n",
       "      <td>2.339000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2260</td>\n",
       "      <td>2.402500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2280</td>\n",
       "      <td>2.365300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>2.351900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2320</td>\n",
       "      <td>2.344800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2340</td>\n",
       "      <td>2.348300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>2.375400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2380</td>\n",
       "      <td>2.369000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.362900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2420</td>\n",
       "      <td>2.359900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2440</td>\n",
       "      <td>2.352300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2460</td>\n",
       "      <td>2.311500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2480</td>\n",
       "      <td>2.348400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.302500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2520</td>\n",
       "      <td>2.368800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2540</td>\n",
       "      <td>2.328300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2560</td>\n",
       "      <td>2.370400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2580</td>\n",
       "      <td>2.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.379800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2620</td>\n",
       "      <td>2.378700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2640</td>\n",
       "      <td>2.378600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2660</td>\n",
       "      <td>2.347600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2680</td>\n",
       "      <td>2.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>2.341400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2720</td>\n",
       "      <td>2.393300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2740</td>\n",
       "      <td>2.340700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2760</td>\n",
       "      <td>2.331400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2780</td>\n",
       "      <td>2.304900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>2.348800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2820</td>\n",
       "      <td>2.320100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2840</td>\n",
       "      <td>2.380400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2860</td>\n",
       "      <td>2.336100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2880</td>\n",
       "      <td>2.370500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>2.319600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2920</td>\n",
       "      <td>2.329200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2940</td>\n",
       "      <td>2.335600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2960</td>\n",
       "      <td>2.336000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2980</td>\n",
       "      <td>2.345800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.365000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3020</td>\n",
       "      <td>2.345200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3040</td>\n",
       "      <td>2.303200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3060</td>\n",
       "      <td>2.307700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3080</td>\n",
       "      <td>2.287600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>2.295800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3120</td>\n",
       "      <td>2.322100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3140</td>\n",
       "      <td>2.326900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3160</td>\n",
       "      <td>2.322700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3180</td>\n",
       "      <td>2.350900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>2.310900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3220</td>\n",
       "      <td>2.348800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3240</td>\n",
       "      <td>2.337100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3260</td>\n",
       "      <td>2.296100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3280</td>\n",
       "      <td>2.317000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>2.321000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3320</td>\n",
       "      <td>2.283100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3340</td>\n",
       "      <td>2.333200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3360</td>\n",
       "      <td>2.298300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3380</td>\n",
       "      <td>2.358200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>2.355000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3420</td>\n",
       "      <td>2.317400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3440</td>\n",
       "      <td>2.301700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3460</td>\n",
       "      <td>2.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3480</td>\n",
       "      <td>2.344000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.271600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3520</td>\n",
       "      <td>2.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3540</td>\n",
       "      <td>2.298100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3560</td>\n",
       "      <td>2.353600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3580</td>\n",
       "      <td>2.289000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>2.326100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3620</td>\n",
       "      <td>2.286800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3640</td>\n",
       "      <td>2.292900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3660</td>\n",
       "      <td>2.318700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3680</td>\n",
       "      <td>2.309600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>2.316700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3720</td>\n",
       "      <td>2.305600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3740</td>\n",
       "      <td>2.295900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3760</td>\n",
       "      <td>2.294800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3780</td>\n",
       "      <td>2.291500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>2.258100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3820</td>\n",
       "      <td>2.276600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3840</td>\n",
       "      <td>2.244400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3860</td>\n",
       "      <td>2.317500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3880</td>\n",
       "      <td>2.261300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>2.297200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3920</td>\n",
       "      <td>2.341200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3940</td>\n",
       "      <td>2.331500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3960</td>\n",
       "      <td>2.280400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3980</td>\n",
       "      <td>2.346600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.312200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4020</td>\n",
       "      <td>2.297000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4040</td>\n",
       "      <td>2.292700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4060</td>\n",
       "      <td>2.352600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4080</td>\n",
       "      <td>2.294300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>2.270600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4120</td>\n",
       "      <td>2.312700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4140</td>\n",
       "      <td>2.265900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4160</td>\n",
       "      <td>2.303900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4180</td>\n",
       "      <td>2.298100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>2.330200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4220</td>\n",
       "      <td>2.330200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4240</td>\n",
       "      <td>2.324300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4260</td>\n",
       "      <td>2.263100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4280</td>\n",
       "      <td>2.282500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>2.302100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4320</td>\n",
       "      <td>2.265200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4340</td>\n",
       "      <td>2.334800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4360</td>\n",
       "      <td>2.282500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4380</td>\n",
       "      <td>2.303200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>2.291100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4420</td>\n",
       "      <td>2.267400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4440</td>\n",
       "      <td>2.307100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4460</td>\n",
       "      <td>2.209700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4480</td>\n",
       "      <td>2.325000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>2.248300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4520</td>\n",
       "      <td>2.307600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4540</td>\n",
       "      <td>2.302500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4560</td>\n",
       "      <td>2.347300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4580</td>\n",
       "      <td>2.240600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>2.239300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4620</td>\n",
       "      <td>2.266500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4640</td>\n",
       "      <td>2.299000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4660</td>\n",
       "      <td>2.289200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4680</td>\n",
       "      <td>2.291500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>2.236300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4720</td>\n",
       "      <td>2.304000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4740</td>\n",
       "      <td>2.366400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4760</td>\n",
       "      <td>2.309700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4780</td>\n",
       "      <td>2.270800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>2.296500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4820</td>\n",
       "      <td>2.263900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4840</td>\n",
       "      <td>2.278800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4860</td>\n",
       "      <td>2.297500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4880</td>\n",
       "      <td>2.271500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>2.287400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4920</td>\n",
       "      <td>2.316100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4940</td>\n",
       "      <td>2.271700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4960</td>\n",
       "      <td>2.296300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4980</td>\n",
       "      <td>2.250500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>2.268300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5020</td>\n",
       "      <td>2.247800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5040</td>\n",
       "      <td>2.244000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5060</td>\n",
       "      <td>2.287600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5080</td>\n",
       "      <td>2.266700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>2.272800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5120</td>\n",
       "      <td>2.247500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5140</td>\n",
       "      <td>2.246900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5160</td>\n",
       "      <td>2.306800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5180</td>\n",
       "      <td>2.260600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>2.212000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5220</td>\n",
       "      <td>2.277900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5240</td>\n",
       "      <td>2.294900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5260</td>\n",
       "      <td>2.264900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5280</td>\n",
       "      <td>2.237100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>2.265700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5320</td>\n",
       "      <td>2.244200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5340</td>\n",
       "      <td>2.227000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5360</td>\n",
       "      <td>2.287200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5380</td>\n",
       "      <td>2.285900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>2.298100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5420</td>\n",
       "      <td>2.280700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5440</td>\n",
       "      <td>2.279400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5460</td>\n",
       "      <td>2.269500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5480</td>\n",
       "      <td>2.294500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>2.255900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5520</td>\n",
       "      <td>2.303800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5540</td>\n",
       "      <td>2.207500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5560</td>\n",
       "      <td>2.297500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5580</td>\n",
       "      <td>2.289600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>2.187100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5620</td>\n",
       "      <td>2.263200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5640</td>\n",
       "      <td>2.290600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5660</td>\n",
       "      <td>2.306700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5680</td>\n",
       "      <td>2.242400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>2.224500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5720</td>\n",
       "      <td>2.251100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5740</td>\n",
       "      <td>2.320300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5760</td>\n",
       "      <td>2.252300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5780</td>\n",
       "      <td>2.286500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>2.259000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5820</td>\n",
       "      <td>2.252300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5840</td>\n",
       "      <td>2.255000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5860</td>\n",
       "      <td>2.307200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5880</td>\n",
       "      <td>2.300400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>2.266000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5920</td>\n",
       "      <td>2.258800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5940</td>\n",
       "      <td>2.270900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5960</td>\n",
       "      <td>2.339300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5980</td>\n",
       "      <td>2.247300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>2.193200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6020</td>\n",
       "      <td>2.266400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6040</td>\n",
       "      <td>2.264800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6060</td>\n",
       "      <td>2.253400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6080</td>\n",
       "      <td>2.285500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>2.230600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6120</td>\n",
       "      <td>2.286900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6140</td>\n",
       "      <td>2.213100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6160</td>\n",
       "      <td>2.286100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6180</td>\n",
       "      <td>2.268200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>2.240300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6220</td>\n",
       "      <td>2.262100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6240</td>\n",
       "      <td>2.245800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6260</td>\n",
       "      <td>2.242600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6280</td>\n",
       "      <td>2.237400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>2.278800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6320</td>\n",
       "      <td>2.232900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6340</td>\n",
       "      <td>2.256300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6360</td>\n",
       "      <td>2.276600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6380</td>\n",
       "      <td>2.224000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>2.226800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6420</td>\n",
       "      <td>2.238700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6440</td>\n",
       "      <td>2.256600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6460</td>\n",
       "      <td>2.244400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6480</td>\n",
       "      <td>2.275300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>2.239700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6520</td>\n",
       "      <td>2.244100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6540</td>\n",
       "      <td>2.261100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6560</td>\n",
       "      <td>2.245200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6580</td>\n",
       "      <td>2.251000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>2.238900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6620</td>\n",
       "      <td>2.248300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6640</td>\n",
       "      <td>2.237300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6660</td>\n",
       "      <td>2.247100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6680</td>\n",
       "      <td>2.197200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>2.200900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6720</td>\n",
       "      <td>2.213300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6740</td>\n",
       "      <td>2.241800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6760</td>\n",
       "      <td>2.257900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6780</td>\n",
       "      <td>2.250400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>2.227700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6820</td>\n",
       "      <td>2.308400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6840</td>\n",
       "      <td>2.236200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6860</td>\n",
       "      <td>2.238900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6880</td>\n",
       "      <td>2.245900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>2.227100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6920</td>\n",
       "      <td>2.250500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6940</td>\n",
       "      <td>2.210600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6960</td>\n",
       "      <td>2.275600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6980</td>\n",
       "      <td>2.284100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>2.281400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7020</td>\n",
       "      <td>2.239100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7040</td>\n",
       "      <td>2.225700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7060</td>\n",
       "      <td>2.249200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7080</td>\n",
       "      <td>2.242200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>2.286300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7120</td>\n",
       "      <td>2.252200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7140</td>\n",
       "      <td>2.215400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7160</td>\n",
       "      <td>2.241800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7180</td>\n",
       "      <td>2.205500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>2.261200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7220</td>\n",
       "      <td>2.242600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7240</td>\n",
       "      <td>2.228500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7260</td>\n",
       "      <td>2.303400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7280</td>\n",
       "      <td>2.211400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>2.207000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7320</td>\n",
       "      <td>2.206200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7340</td>\n",
       "      <td>2.200200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7360</td>\n",
       "      <td>2.210400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7380</td>\n",
       "      <td>2.211900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>2.293100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7420</td>\n",
       "      <td>2.190400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7440</td>\n",
       "      <td>2.208000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7460</td>\n",
       "      <td>2.218500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7480</td>\n",
       "      <td>2.252200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>2.227500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7520</td>\n",
       "      <td>2.233600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7540</td>\n",
       "      <td>2.209800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7560</td>\n",
       "      <td>2.210600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7580</td>\n",
       "      <td>2.210600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>2.220100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7620</td>\n",
       "      <td>2.205900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7640</td>\n",
       "      <td>2.285200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7660</td>\n",
       "      <td>2.229600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7680</td>\n",
       "      <td>2.276400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>2.285000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7720</td>\n",
       "      <td>2.235700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7740</td>\n",
       "      <td>2.188500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7760</td>\n",
       "      <td>2.211600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7780</td>\n",
       "      <td>2.255300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>2.231100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7820</td>\n",
       "      <td>2.277000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7840</td>\n",
       "      <td>2.234100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7860</td>\n",
       "      <td>2.220600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7880</td>\n",
       "      <td>2.263500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>2.258500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7920</td>\n",
       "      <td>2.215200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7940</td>\n",
       "      <td>2.204000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7960</td>\n",
       "      <td>2.180800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7980</td>\n",
       "      <td>2.300400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>2.253300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8020</td>\n",
       "      <td>2.236500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8040</td>\n",
       "      <td>2.181200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8060</td>\n",
       "      <td>2.218900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8080</td>\n",
       "      <td>2.244800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>2.224000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8120</td>\n",
       "      <td>2.251100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8140</td>\n",
       "      <td>2.167200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8160</td>\n",
       "      <td>2.237000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8180</td>\n",
       "      <td>2.225300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>2.250700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8220</td>\n",
       "      <td>2.204500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8240</td>\n",
       "      <td>2.266500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8260</td>\n",
       "      <td>2.200900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8280</td>\n",
       "      <td>2.221700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>2.214500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8320</td>\n",
       "      <td>2.282100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8340</td>\n",
       "      <td>2.183200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8360</td>\n",
       "      <td>2.249100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8380</td>\n",
       "      <td>2.275500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>2.226000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8420</td>\n",
       "      <td>2.269300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8440</td>\n",
       "      <td>2.243900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8460</td>\n",
       "      <td>2.220100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8480</td>\n",
       "      <td>2.290400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>2.258800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8520</td>\n",
       "      <td>2.264000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8540</td>\n",
       "      <td>2.240200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8560</td>\n",
       "      <td>2.230400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8580</td>\n",
       "      <td>2.225300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>2.178100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8620</td>\n",
       "      <td>2.225700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8640</td>\n",
       "      <td>2.243500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8660</td>\n",
       "      <td>2.247100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8680</td>\n",
       "      <td>2.234700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>2.257700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8720</td>\n",
       "      <td>2.226900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8740</td>\n",
       "      <td>2.240400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8760</td>\n",
       "      <td>2.227100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8780</td>\n",
       "      <td>2.211900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>2.213700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8820</td>\n",
       "      <td>2.263000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8840</td>\n",
       "      <td>2.294500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8860</td>\n",
       "      <td>2.259900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8880</td>\n",
       "      <td>2.212300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>2.251900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8920</td>\n",
       "      <td>2.200200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8940</td>\n",
       "      <td>2.240500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8960</td>\n",
       "      <td>2.213600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8980</td>\n",
       "      <td>2.237300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>2.192700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9020</td>\n",
       "      <td>2.216400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9040</td>\n",
       "      <td>2.201400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9060</td>\n",
       "      <td>2.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9080</td>\n",
       "      <td>2.204200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>2.273600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9120</td>\n",
       "      <td>2.227000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9140</td>\n",
       "      <td>2.233800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9160</td>\n",
       "      <td>2.223300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9180</td>\n",
       "      <td>2.218900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>2.237000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9220</td>\n",
       "      <td>2.217600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9240</td>\n",
       "      <td>2.269400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9260</td>\n",
       "      <td>2.253200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9280</td>\n",
       "      <td>2.241200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>2.277200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9320</td>\n",
       "      <td>2.218700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9340</td>\n",
       "      <td>2.258000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9360</td>\n",
       "      <td>2.205100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Fatal error while uploading data. Some run data will not be synced, but it will still be written to disk. Use `wandb sync` at the end of the run to try uploading.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9375, training_loss=2.316283463948568, metrics={'train_runtime': 28778.4043, 'train_samples_per_second': 5.212, 'train_steps_per_second': 0.326, 'total_flos': 1.47043326035755e+18, 'train_loss': 2.316283463948568, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"../output/medical_lora_adapter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 推理测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: 红楼梦的作者是谁\n",
      "\n",
      "Assistant: 鲁迅。鲁迅的小说都是反映中国社会现状，写得很深刻的一些描绘。在我看来，你说的红楼梦是作品中的一个虚构故事吗？鲁迅还有其它小说吗？\n",
      "\n",
      "这篇文章里面好像很多东西都挺有启发性的，不过我没弄清楚具体情况。\n",
      "\n",
      "关于“鲁迅”，你知道吗？\n",
      "\n",
      "根据我的了解，\"鲁迅\"指的是马自远，他用字号别名鲁迅，即字号鲁文旦。\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Human: 红楼梦的作者是谁\\n\\nAssistant: \"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=1024,  # 允许回答更长\n",
    "    temperature=1.0,  # 让回答更加自由\n",
    "    top_p=0.9,  # 让模型考虑更多可能性\n",
    "    top_k=50,  # 增加多样性\n",
    "    repetition_penalty=1.2,  # 防止重复\n",
    "    no_repeat_ngram_size=3,  # 避免生成相同的三元组短语\n",
    "    eos_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers_1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
